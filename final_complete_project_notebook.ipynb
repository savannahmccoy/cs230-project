{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CS 230 Deep Learning: Final Project  \n",
    "### Exploration of User Privacy Preservation via CTGAN Data Synthesis for Deep Recommenders\n",
    "\n",
    "---\n",
    "**Contributors:** Savannah McCoy\n",
    "\n",
    "**In this notebook we will generate synthetic user review data and train several deep recommenders using various split of real and synthetic data**\n",
    "\n",
    "_(The outputs of this notebook have been cleared prior to upload for easy reading. Full outputs for all cells of the following code can be found in the notebooks in the **dev-notebooks** directory)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import missingno as msno\n",
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from typing import Dict, Text\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from tabgan.sampler import GANGenerator, OriginalGenerator \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformatting json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Video_Games_5.json\", sep=\"\\n\", header=None)\n",
    "d = {}\n",
    "def split_data(row):\n",
    "    dt = json.loads(row[0])\n",
    "    d[row.name] = dt\n",
    "    return\n",
    "\n",
    "df.apply(lambda row: split_data(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    json.dump(d, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"meta_Video_Games.json\", sep=\"\\n\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = {}\n",
    "def split_data(row):\n",
    "    dt = json.loads(row[0])\n",
    "    dp[row.name] = dt\n",
    "    return\n",
    "\n",
    "df3.apply(lambda row: split_data(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"meta.json\", \"w\") as outfile:\n",
    "    json.dump(dp, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read reformatted json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_json('meta_sub.json')\n",
    "reviews_df = pd.read_json('reviews.json')\n",
    "reviews_df = reviews_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine missing data\n",
    "msno.matrix(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data and encoding categorial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_json('reviews.json')\n",
    "reviews_df = reviews_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reviews_df[\"reviewTime\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.rename(columns={\"overall\": \"rating\", \n",
    "                           \"reviewerID\": \"userID\", \n",
    "                           \"asin\": \"productID\"}, \n",
    "                  errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_csv(\"reviews_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv('reviews_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reviews_df[\"style\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['vote'] = reviews_df['vote'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(reviews_df[\"vote\"])\n",
    "new_vote = le.transform(reviews_df[\"vote\"])\n",
    "reviews_df[\"cat_vote\"] = new_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_rating(x):\n",
    "    if x >= 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "reviews_df[\"bin_rating\"] = reviews_df[\"rating\"].apply(lambda x: bin_rating(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_verified(x):\n",
    "    if x:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "reviews_df[\"bin_verified\"] = reviews_df[\"verified\"].apply(lambda x: bin_verified(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reviews_df[\"rating\"]\n",
    "del reviews_df[\"verified\"]\n",
    "del reviews_df[\"vote\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_csv(\"reviews_categorical.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv('reviews_categorical.csv')\n",
    "reviews_df.columns = [\"userID\", \"productID\", \"vote\", \"rating\", \"verified\"]\n",
    "reviews_df = reviews_df.sample(n=100000, replace=False, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabgan==1.1.0\n",
    "# !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = pd.DataFrame(columns =[\"userID\", \"productID\", \"vote\", \"verified\"])\n",
    "tsr = pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv('reviews_categorical.csv')\n",
    "reviews_df.columns = [\"userID\", \"productID\", \"vote\", \"rating\", \"verified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(125):\n",
    "    reviews_df_sub = reviews_df.sample(frac=0.03, replace=False, random_state=i)\n",
    "    df_train, df_test = train_test_split(reviews_df_sub, test_size=0.2)\n",
    "    df_train2 = df_train[[\"userID\", \"productID\", \"vote\", \"verified\"]]\n",
    "    df_test2 = df_test[[\"userID\", \"productID\", \"vote\", \"verified\"]]\n",
    "    df_target = df_train[[\"rating\"]]\n",
    "    new_train, new_target = GANGenerator(cat_cols=[\"userID\", \"productID\", \"vote\", \n",
    "                                                    \"verified\", \"rating\"], \n",
    "                                          is_post_process=True, epochs=3\n",
    "                                          ).generate_data_pipe(train_df=df_train2, \n",
    "                                                                test_df=df_test2, \n",
    "                                                                target=df_target, \n",
    "                                                                only_adversarial=False,\n",
    "                                                                use_adversarial=True,\n",
    "                                                                )\n",
    "    rdf = pd.concat([rdf, new_train])  \n",
    "    tsr = pd.concat([tsr, new_target])\n",
    "    print(\"DONE: Iteration\", i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging meta data to Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_json('meta.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df.T\n",
    "meta_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del meta_df[\"tech1\"]\n",
    "del meta_df[\"fit\"] \n",
    "del meta_df[\"also_buy\"] \n",
    "del meta_df[\"tech2\"] \n",
    "del meta_df[\"rank\"] \n",
    "del meta_df[\"also_view\"] \n",
    "del meta_df[\"main_cat\"] \n",
    "del meta_df[\"similar_item\"] \n",
    "del meta_df[\"imageURL\"]\n",
    "del meta_df[\"imageURLHighRes\"]\n",
    "del meta_df[\"details\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['price'] = meta_df['price'].astype(str)\n",
    "meta_df['brand'] = meta_df['brand'].astype(str)\n",
    "meta_df['category'] = meta_df['category'].astype(str)\n",
    "meta_df['description'] = meta_df['description'].astype(str)\n",
    "meta_df['date'] = meta_df['date'].astype(str)\n",
    "meta_df['title'] = meta_df['title'].astype(str)\n",
    "meta_df['feature'] = meta_df['feature'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_prices(x):\n",
    "    if x.startswith('$'):\n",
    "        return x\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "meta_df[\"price\"] = meta_df[\"price\"].apply(lambda x: filter_prices(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(meta_df[\"price\"])\n",
    "new_price = le.transform(meta_df[\"price\"])\n",
    "meta_df[\"price\"] = new_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(meta_df[\"date\"])\n",
    "new_date = le.transform(meta_df[\"date\"])\n",
    "meta_df[\"date\"] = new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['description'] = meta_df['description'].str.strip('[]').str.split(',')\n",
    "meta_df['feature'] = meta_df['feature'].str.strip('[]').str.split(',')\n",
    "meta_df['category'] = meta_df['category'].str.strip('[]').str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat2(x):\n",
    "    if len(x) >= 2:\n",
    "        return x[1].replace(\"'\", '')\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def get_cat3(x):\n",
    "    if len(x) >= 3:\n",
    "        return x[2].replace(\"'\", '')\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def get_description(x):\n",
    "    if len(x) >= 1:\n",
    "        return x[0].replace(\"'\", '')\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_feature(x):\n",
    "    if len(x) >= 1:\n",
    "        return x[0].replace(\"'\", '')\n",
    "    else:\n",
    "        return ''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[\"description\"] = meta_df[\"description\"].apply(lambda x: get_description(x))\n",
    "meta_df[\"feature\"] = meta_df[\"feature\"].apply(lambda x: get_feature(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del meta_df[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df.rename(columns={\"asin\": \"productID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(meta_df, reviews_df, on='productID')\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result.sample(n=100000, replace=False, random_state=63)\n",
    "result_df.to_csv(\"100_real_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read full metadata data set\n",
    "meta_df = pd.read_json('meta.json')\n",
    "meta_df = meta_df.T\n",
    "\n",
    "# process metadata to be merged with split data\n",
    "def filter_prices(x):\n",
    "    if x.startswith('$'):\n",
    "        return x\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "del meta_df[\"tech1\"]\n",
    "del meta_df[\"fit\"] \n",
    "del meta_df[\"also_buy\"] \n",
    "del meta_df[\"tech2\"] \n",
    "del meta_df[\"rank\"] \n",
    "del meta_df[\"also_view\"] \n",
    "del meta_df[\"main_cat\"] \n",
    "del meta_df[\"similar_item\"] \n",
    "del meta_df[\"imageURL\"]\n",
    "del meta_df[\"imageURLHighRes\"]\n",
    "del meta_df[\"details\"]\n",
    "del meta_df[\"description\"] \n",
    "del meta_df[\"feature\"]\n",
    "del meta_df[\"title\"]\n",
    "del meta_df[\"brand\"]\n",
    "del meta_df[\"category\"]\n",
    "\n",
    "meta_df['price'] = meta_df['price'].astype(str)\n",
    "meta_df['date'] = meta_df['date'].astype(str) \n",
    "meta_df[\"price\"] = meta_df[\"price\"].apply(lambda x: filter_prices(x))\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(meta_df[\"price\"])\n",
    "new_price = le.transform(meta_df[\"price\"])\n",
    "meta_df[\"price\"] = new_price\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(meta_df[\"date\"])\n",
    "new_date = le.transform(meta_df[\"date\"])\n",
    "meta_df[\"date\"] = new_date\n",
    "\n",
    "meta_df = meta_df.rename(columns={\"asin\": \"productID\"})\n",
    "meta_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in real and synthetic datasets\n",
    "real_df = pd.read_csv('reviews_categorical.csv')\n",
    "real_df.columns = [\"userID\", \"productID\", \"vote\", \"rating\", \"verified\"]\n",
    "syn_df = pd.read_csv('synthetic_review_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 Synth\n",
    "syn_df = syn_df.rename(columns={\"0\": \"rating\"})\n",
    "syn_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge metadata and full synth \n",
    "result1 = pd.merge(meta_df, syn_df, on='productID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save synthetic dataset\n",
    "synth_100 = result1.sample(n=100000, replace=False, random_state=35)\n",
    "synth_100.to_csv(\"100_synth_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 75/25 split\n",
    "real_df1 = real_df.sample(n=75000, replace=False, random_state=3)\n",
    "syn_df1 = syn_df.sample(n=25000, replace=False, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_75_25 = pd.concat([real_df1, syn_df1])\n",
    "df_75_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = pd.merge(meta_df, df_75_25, on='productID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 75/25 dataset\n",
    "real_75 = result2.sample(n=100000, replace=False, random_state=35)\n",
    "real_75.to_csv(\"75_real_25_synth_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 50/50 split\n",
    "real_df2 = real_df.sample(n=50000, replace=False, random_state=43)\n",
    "syn_df2 = syn_df.sample(n=50000, replace=False, random_state=35)\n",
    "df_50_50 = pd.concat([real_df2, syn_df2])\n",
    "result3 = pd.merge(meta_df, df_50_50, on='productID')\n",
    "print(len(result3))\n",
    "synth_50 = result3.sample(n=100000, replace=False, random_state=35)\n",
    "synth_50.to_csv(\"50_real_50_synth_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 90/10 split\n",
    "real_df3 = real_df.sample(n=90000, replace=False, random_state=43)\n",
    "syn_df3 = syn_df.sample(n=10000, replace=False, random_state=35)\n",
    "df_90_10 = pd.concat([real_df3, syn_df3])\n",
    "result4 = pd.merge(meta_df, df_90_10, on='productID')\n",
    "print(len(result4))\n",
    "synth_10 = result4.sample(n=100000, replace=False, random_state=35)\n",
    "synth_10.to_csv(\"90_real_10_synth_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Recommender Models on datatset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q tensorflow-recommenders\n",
    "# ! pip install -q --upgrade tensorflow-datasets\n",
    "# ! pip install latex\n",
    "# ! sudo apt-get install texlive-latex-recommended \n",
    "# ! sudo apt install texlive-latex-extra\n",
    "# ! sudo apt install dvipng\n",
    "# ! sudo apt-get install texlive-latex-extra texlive-fonts-recommended dvipng cm-super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a user model\n",
    "class UserModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        max_tokens = 10_000\n",
    "        self.embedding_dimension = 32\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.StringLookup(vocabulary=u_uids, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(u_uids) + 1, 64)])\n",
    "\n",
    "        self.vote_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.Discretization(vote_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(vote_buckets) + 2, 32)])\n",
    "        self.normalized_vote = tf.keras.layers.Normalization(axis=None)\n",
    "        self.normalized_vote.adapt(votes)\n",
    "\n",
    "        self.verified_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.Discretization(verified_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(verified_buckets) + 2, 32)])\n",
    "        self.normalized_verified = tf.keras.layers.Normalization(axis=None)\n",
    "        self.normalized_verified.adapt(verified)\n",
    "\n",
    "        self.date_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.Discretization(date_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(date_buckets) + 2, 32)])\n",
    "        self.normalized_date = tf.keras.layers.Normalization(axis=None)\n",
    "        self.normalized_date.adapt(votes)\n",
    "\n",
    "        self.price_embedding = tf.keras.Sequential([\n",
    "            tf.keras.layers.Discretization(price_buckets.tolist()),\n",
    "            tf.keras.layers.Embedding(len(price_buckets) + 2, 32)])\n",
    "        self.normalized_price = tf.keras.layers.Normalization(axis=None)\n",
    "        self.normalized_price.adapt(prices)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.concat([\n",
    "            self.user_embedding(inputs[\"userID\"]),\n",
    "            self.vote_embedding(inputs[\"vote\"]),\n",
    "            tf.reshape(self.normalized_vote(inputs[\"vote\"]), (-1, 1)),\n",
    "            self.verified_embedding(inputs[\"verified\"]),\n",
    "            tf.reshape(self.normalized_verified(inputs[\"verified\"]), (-1, 1)),\n",
    "            self.date_embedding(inputs[\"date\"]),\n",
    "            tf.reshape(self.normalized_verified(inputs[\"date\"]), (-1, 1)),\n",
    "            self.price_embedding(inputs[\"price\"]),\n",
    "            tf.reshape(self.normalized_vote(inputs[\"price\"]), (-1, 1)),\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a product model\n",
    "class ProductModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.product_embedding = tf.keras.Sequential([\n",
    "                      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                          vocabulary=u_pids, mask_token=None),\n",
    "                      tf.keras.layers.Embedding(len(u_pids) + 1, 64)])\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(inputs)\n",
    "        return tf.concat([ \n",
    "            self.product_embedding(inputs[\"productID\"])\n",
    "        ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a recommender model\n",
    "class ProductRecommendationModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, rating_weight, retrieval_weight):\n",
    "        super().__init__()\n",
    "\n",
    "        # user and product representations\n",
    "        self.user_model = tf.keras.Sequential([\n",
    "                          UserModel(),\n",
    "                          tf.keras.layers.Dense(64)])\n",
    "\n",
    "        ## candidate model is the item model\n",
    "        self.product_model = tf.keras.Sequential([\n",
    "                              ProductModel(),\n",
    "                              tf.keras.layers.Dense(64)])\n",
    "\n",
    "\n",
    "        # model using user and product embeddings to predict ratings\n",
    "        self.rating_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ])\n",
    "\n",
    "        # loss weights\n",
    "        self.rating_weight = rating_weight\n",
    "        self.retrieval_weight = retrieval_weight\n",
    "\n",
    "        # retrieval tasks: factorization loss and RMSE\n",
    "        self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "        )\n",
    "        self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=products.batch(128).map(self.product_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        # define how the loss is computed\n",
    "        ratings = features.pop(\"rating\")\n",
    "\n",
    "        user_embeddings = self.user_model({\n",
    "            \"userID\": features[\"userID\"],\n",
    "            \"vote\": features[\"vote\"],\n",
    "            \"verified\": features[\"verified\"],\n",
    "            \"date\": features[\"date\"],\n",
    "            \"price\": features[\"price\"], \n",
    "        })\n",
    "\n",
    "        product_embeddings = self.product_model({\n",
    "            \"productID\": features[\"productID\"],  \n",
    "        })\n",
    "\n",
    "        rating_predictions = self.rating_model(\n",
    "            tf.concat([user_embeddings, product_embeddings], axis=1)\n",
    "        )  \n",
    "\n",
    "        # compute loss for each task\n",
    "        rating_loss = self.rating_task(labels=ratings, predictions=rating_predictions)\n",
    "        retrieval_loss = self.retrieval_task(user_embeddings, product_embeddings)\n",
    "\n",
    "        # combine them using the loss weights\n",
    "        return (self.rating_weight * rating_loss + self.retrieval_weight * retrieval_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets to pandas dfs\n",
    "df1 = pd.read_csv('100_synth_data.csv')\n",
    "df2 = pd.read_csv('90_real_10_synth_data.csv')\n",
    "df3 = pd.read_csv('75_real_25_synth_data.csv')\n",
    "df4 = pd.read_csv('50_real_50_synth_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of dfs to iterate over\n",
    "dfs = [df1, df2, df3, df4]\n",
    "strps = [\"100_synth\", \"90_real\", \"75_real\", \"50_real\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for df in dfs:\n",
    "    # read data from csv files\n",
    "    reviews_df = df\n",
    "\n",
    "    # convert dataframe to tensor dataset format\n",
    "    reviews = tf.data.Dataset.from_tensor_slices(dict(reviews_df))\n",
    "\n",
    "    # select the features to map\n",
    "    reviews = reviews.map(lambda x: {\n",
    "        \"productID\": x[\"productID\"],      # embedding\n",
    "        \"userID\": x[\"userID\"],            # embedding\n",
    "        \"rating\": x[\"rating\"],            # target\n",
    "        \"vote\": x[\"vote\"],                # numeric\n",
    "        \"date\": x[\"date\"],                # numeric\n",
    "        \"price\": x[\"price\"],              # numeric\n",
    "        \"verified\": x[\"verified\"]         # numeric\n",
    "        })\n",
    "\n",
    "    products = reviews.map(lambda x: {\n",
    "        \"productID\": x[\"productID\"]})\n",
    "    \n",
    "    start1 = time.time()\n",
    "\n",
    "    # process numeric inputs\n",
    "    votes = np.concatenate(list(reviews.map(lambda x: x[\"vote\"]).batch(100)))\n",
    "    mx_v, mn_v = votes.max(), votes.min()\n",
    "    vote_buckets = np.linspace(mn_v, mx_v, num=1000)\n",
    "\n",
    "    prices = np.concatenate(list(reviews.map(lambda x: x[\"price\"]).batch(100)))\n",
    "    mx_p, mn_p = prices.max(), prices.min()\n",
    "    price_buckets = np.linspace(mn_p, mx_p, num=1000)\n",
    "\n",
    "    verified = np.concatenate(list(reviews.map(lambda x: x[\"verified\"]).batch(100)))\n",
    "    mx_v, mn_v = verified.max(), verified.min()\n",
    "    verified_buckets = np.linspace(mn_v, mx_v, num=1000)\n",
    "\n",
    "    dates = np.concatenate(list(reviews.map(lambda x: x[\"date\"]).batch(100)))\n",
    "    mx_d, mn_d = dates.max(), dates.min()\n",
    "    date_buckets = np.linspace(mn_d, mx_d, num=1000)\n",
    "\n",
    "    # process string inputs for embeddings\n",
    "    u_pids = np.unique(np.concatenate(list(products.batch(1000).map(lambda x: x[\"productID\"]))))\n",
    "    u_uids = np.unique(np.concatenate(list(reviews.batch(1000).map(lambda x: x[\"userID\"]))))\n",
    "\n",
    "    # create retrieval and ranking model\n",
    "    model = ProductRecommendationModel(rating_weight=0.5, retrieval_weight=0.5)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "    # split the data into a training set and a testing set.\n",
    "    tf.random.set_seed(42)\n",
    "    shuffled = reviews.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "    train = shuffled.take(80_000)\n",
    "    test = shuffled.skip(80_000).take(20_000)\n",
    "\n",
    "    cached_train = train.shuffle(100_000).batch(2048)\n",
    "    cached_test = test.batch(4096).cache()\n",
    "\n",
    "    # train model\n",
    "    start = time.time()\n",
    "    history = model.fit(cached_train, epochs=10)\n",
    "    end = time.time()\n",
    "    print(\"TIME TO TRAIN MODEL:\", str(datetime.timedelta(seconds=end-start)))\n",
    "\n",
    "    # print result metrics\n",
    "    start = time.time()\n",
    "    model.evaluate(cached_test, return_dict=True)\n",
    "    end = time.time()\n",
    "    print(\"TIME TO EVALUATE MODEL:\", str(datetime.timedelta(seconds=end-start)))\n",
    "    end1 = time.time()\n",
    "    print(\"\\n\\n\\n ----------------------------------------------------- \\n\\nTOTAL TIME TO TRAIN MODEL:\", str(datetime.timedelta(seconds=end1-start1)))\n",
    "\n",
    "    # plot train accuracy over epochs \n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    mpl.rcParams['text.usetex'] = True\n",
    "    plt.rcParams['text.latex.preamble']=[r\"\\usepackage{lmodern}\"]\n",
    "    params = {'text.usetex' : True,\n",
    "              'font.size' : 11,\n",
    "              'font.family' : 'lmodern',\n",
    "              'text.latex.unicode': True,\n",
    "              }\n",
    "    plt.rcParams.update(params) \n",
    "    fig = plt.figure()''/''\n",
    "    strp = strps[i]\n",
    "    epochs = [i for i in range(10)]\n",
    "    plt.plot(epochs, history.history[\"factorized_top_k/top_100_categorical_accuracy\"], label=\"accuracy\")\n",
    "    plt.title(\"Factorized Top-100 Categorical Over Epochs\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\" accuracy\");\n",
    "    plt.legend()\n",
    "    plt.savefig(strp+\"graph.pdf\")\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rating_model.summary()\n",
    "model.user_model.summary()\n",
    "model.product_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
